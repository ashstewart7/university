% !TeX root = main.tex
\lecture{6}{Thu 16 Oct 2025 09:00}{Likelihood and Log Likelihood}

\section{Likelihood}
We want to fit a model to our data. We want some kind of function to specify how well this model fits the data, so that we can optimise to find the best. This is the likelihood function. There are many different ways to formulate it, but we denote it:
\[
    P(D \, | \, \theta)
\]

Where D is our data, and $\theta$ is our model parameters. This is the probability of the data, given some parameters.

\section{An Example}
Lets say we have this model:
\[
    T(t) = T_\text{env} + (T_0 - T_\text{env}) \exp(-t / \tau)
\]
Which represents the cooling of an object, where $\tau$ is a constant of cooling. We think we will observe some additive, normally distributed noise on these measurements, giving us:
\[
    T_\text{obs}(t) = T(t) + \epsilon
\]

We may observe something like this, where the blue dots are the model-predicted values and the observed data with error noise is in black:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/lec06-01.png}
     \caption{Simulated Data}
\end{figure}

Given the noise is normally distributed, we would expect the true values to lie within the error bars of our observation about 68\% of the time. We see this approximately here. How can we then fit a model to this data? We need to:
\begin{enumerate}
    \item Formulate a model.
    \item Estimate a probability that the model is correct.
\end{enumerate}
 
Given we now have data, and some model we would like to try to fit the data to (we want to fit it to Newton's Law of Cooling, the model previously, and determine and appropriate value of parameters and $\tau$). We therefore want to find a `merit function' to describe how good a fit any model we might create is. We start from the probability of getting some value of the noise.

As a reminder, our model is, noting we are treating time as a discrete set of times, indexed by $i$:
\[
    M(t_i, \theta) = T_\text{env} + (T_0 - T_\text{env}) \exp(-t_i / \tau)
\]

And the probability of getting some value of the noise on the $i$th measurement is (note the first equality, where we can also write it ignoring theta, because noise is independent of the parameters):
\[
    P(\epsilon_i | \theta) = P(\epsilon_i) = \frac{1}{\sigma_{D_i} \sqrt{2 \pi}} \exp \left(\frac{-\epsilon_i^2}{2 \sigma^2_{D_i}}\right)
\]
This is a Normal distribution with a mean of zero, and a standard deviation of $\sigma_{D_i}$

We cannot directly measure $\epsilon_i$, but we know it is the difference between the measured value in the data and the `true' value predicted by our model:
\[
    \epsilon_i = D_i - M(t_i, \theta)
\]

Since the noise is additive and Normal, we can combine these two equations to get our merit function - the probability of a single observed data point given the parameters as:
\[
    P(D_i | \theta) = \frac{1}{\sigma_{D_i} \sqrt{2 \pi}} \exp{\left(\frac{-(D_i - M(t_i, \theta))^2}{2 \sigma^2_{D_i}}\right)}
\]

Where $\theta$ is our list of parameters $\theta = [T_0, T_\text{env}, \tau]$. Crucially, this is a Normal distribution where the mean is our model's prediction given the parameters, and the standard deviation is the uncertainty on the error point. Assuming we have multiple uncorrelated data points, the total likelihood function is:
\[
    P(D | \theta) = \prod_{i=1}^{n} P(D_i | \theta)
\]

Considering $\tau$ as the variable we actually change, we get:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/lec06-02.png}
     \caption{A heatmap of the likelihood function overlaid on the data.}
\end{figure}

The high likelihood values of $P(D|\tau)$ are the models which are most likely to generate the observed data, given the parameters. This, therefore, means that they are the models which best fit the data.

If we plot $P(D, \tau)$ against $\tau$, we can see that the likelihood does a reasonable job of giving us a value which is close to true:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/lec06-03.png}
     \caption{}
\end{figure}

We can use Maximum Likelihood Estimation (note that it is just an estimate, the value of tau given by the maximum likelihood and the true value are \textbf{not} the same) to estimate the best value of tau for the model. We chose the value of tau that gives the maximum likelihood:
\[
    \hat{\tau} = \arg\max_{\tau} P(D|\tau)
\]

In general, given a set of multiple parameters, we say:
\[
    \hat{\theta} = \arg\max_{\theta} P(D|\theta)
\]

\section{Log Likelihood}
We still need to estimate the uncertainty on this predicted best value of tau. It turns out that a good way to do this is by taking the log likelihood instead of just the likelihood.
