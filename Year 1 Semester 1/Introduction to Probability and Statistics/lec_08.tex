% !TeX root = main.tex
\lecture{8}{Thu 23 Oct 2025 09:00}{Fitting a Straight Line 2}

\section{Uncertainties on Best Fit Parameters}
\subsection{What do these uncertainties actually mean?}
Previously, we found that the likelihood of a single value could be described by a Normal distribution.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec08-01.png}
     \caption{}
\end{figure}

We can therefore quote the best fit value as $\hat{\tau} = 3.227 \pm 0.133$. What we're effectively saying is that our 1 sigma uncertainty encompasses 68\% of the probability density such that:
\[
\int_{\hat{\tau}-\sigma_{\hat{\tau}}}^{\hat{\tau}+\sigma_{\hat{\tau}}} P(D\mid\tau)\,d\tau \approx 0.68
\]
And the same for 2 sigma uncertainty with 0.95, and 2 sigma uncertainty with 0.997. We can therefore say that while it may be common for the value to lie outside the 1 sigma uncertainty, it is rare for it to lie outside the 3 sigma uncertainty and if this happens (including error), something probably went wrong with our measurement. 

Here, where we're attempting to determine an uncertainty, we instead increase the value of $\sigma_{\hat{\tau}}$ until this first integral is satisfied.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec08-02.png}
     \caption{}
\end{figure}

However, with a line of best fit, we want to consider uncertainties in two dimensions. This makes life a little bit more difficult, we want some boundary on the parameter space that is centred on the best fit parameters and encompasses 68\% of the whole probability space.

Plotting the same distribution again, but including the 1D likelihood for each parameter individually:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec08-03.png}
     \caption{}
\end{figure}

In general, we shouldn't just separately calculate the 1 dimensional approach per parameter and combine them (as the parameters may be correlated), but it's a useful starting point. Swapping to log likelihood:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec08-04.png}
     \caption{}
\end{figure}

We want to estimate:
\[
    \frac{\partial^2 \LL}{\partial m^2} \biggr\rvert_{\theta = \hat{\theta}}
\]
\[
    \frac{\partial^2 \LL}{\partial c^2} \biggr\rvert_{\theta = \hat{\theta}}
\]

While we shouldn't use these 1D distributions to estimate the best fit parameters, this problem has deliberately been created to minimise the correlation between $m$ and $c$ and creating something with them that describes $P(D\mid\theta)$ is still instructive.

We assume that $P(D\mid\theta)$ can be described by a two dimensional normal distribution, and we can  build this from two discrete normal distributions of two independent variables (with the caveats above). We say that the mean value of $P(D\mid m)$ is $\hat{m}$ and the uncertainty is therefore given by:
\[
    \sigma^2_m = \left(-\frac{\partial^2 \LL}{\partial m^2} \biggr\rvert_{\theta = \hat{\theta}} \right)^{-1}
\]
And likewise for $c$, $\hat{c}$, $\sigma_c^2$
\[
    \sigma^2_c = \left(-\frac{\partial^2 \LL}{\partial c^2} \biggr\rvert_{\theta = \hat{\theta}} \right)^{-1}
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec08-05.png}
     \caption{}
\end{figure}

Calculating our summary stats and plotting them:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec08-06.png}
     \caption{}
\end{figure}
We can see that this is pretty good agreement between our estimate and the actual log likelihood function. We now have everything we need to quote the best fit parameters and (crucially) their uncertainty.