% !TeX root = main.tex
\lecture{5}{Wed 15 Oct 2025 12:00}{Distributions}

\section{Coin Flips and Probability Recap}
Flipping a coin is one of the simplest distributions we can create.

Given a:
\begin{align*}
P(H) &= 0.5\\
P(T) &= 0.5
\end{align*}

We know that $P(HHHH) = 0.5^4$.

And $P(\text{Three Heads and One Tail}) = P(\text{HHHT or HHTH or THHH or HTHH}) = 4 \times 0.5^4$. We will take 4 coins, A, B, C, D. We denote a single result as $A_\text{Heads}$ or $C_\text{Tails}$ etc.

We can also say that the coins are independent, i.e. the probability of one result given another result is equal to just the probability of the first result:
\[
    P(A_h | B_h) = 0.5
\]

The chance of A and B being heads is:
\[
    P(A_h \text{and} B_h) = P(A_h) \times P(B_h) = P(HH)
\]

The chance of A \emph{or} B being heads is (noting \emph{or} excludes the case where both are true):
\[
    P(A_h \text{ or } B_h) = P(A_h) + P(B_h) - P(A_h \text{ and } B_h)
\]

\subsection{Discrete Distribution}
Lets consider flipping 4 coins and counting the number of heads. This forms a discrete distribution (where only 5 possible values are possible, 0, 1, 2, 3, 4). This distribution must be normalised (sum to 1), so:
\[
    \sum_r P(r) = 1
\]

We can also consider the mean (expected) number of heads:
\[
    \langle r\rangle = \sum_{r} r P(r)
\]

This function, $P(x)$ is called a \emph{probability mass function}, and the sum of all values must be 1.

\subsection{Continuous Distributions}
Continuous distributions have similar conditions:
\[
    \int_{-\infty}^{\infty} P(x) dx =1
\]

\[
    \langle x\rangle = \int_{-\infty}^{\infty} xP(x) \, dx
\]

And for the probability of the result lying between a and b:

\[
    \int_{a}^{b} P(x) \, dx
\]

We cannot, in a continuous distribution consider the probability of an exact result, i.e. $P(x = a)$, $a \in \R$. As there are infinitely many possible values, the probability of any precise one is not meaningful (always zero). We therefore must always consider the probability of the result lying in some non-zero range. 

P(x) in this case is called a \emph{probability density function} and the area under the PDF curve must sum to one. Note that this means that P(x) at any point may exceed one, so long as the overall area is equal to 1. For some probability $a < P(x) < b$ (noting that since the P(a) for any precise a is zero, the equalities being strict or not is meaningless), the probability is the area of the curve between a and b.

\section{Binomial Distribution}
The Binomial Distribution represents a scenario where we conduct some number of identical trials, where each trial has two possible outcomes (which we denote success and failure). For example, flipping a coin. Here:
\begin{itemize}
    \item $n$ - The number of trials.
    \item $p$ - The probability of success.
    \item $q$ - The probability of failure ($q = 1 - p$).
    \item $r$ - The number of successes.
\end{itemize}

This has probability mass function:
\[
    P(r; n, p) = \frac{n!}{r!(n-r)! }p^r (1-p)^{n-r}
\]

And has the following properties:
\[
    \langle r\rangle = np
\]
\[
    Var(r) = np(1-p)
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/lec05-01.png}
     \caption{The Binomial Distribution}
\end{figure}
\section{Poisson Distribution}
This describes the number of events (i.e. the number of neutrinos detected by a neutrino detector) occurring in some time interval, given:
\begin{itemize}
    \item The mean rate of events is constant.
    \item Each event occurs independently from the last.
\end{itemize}

This is created by taking the limit of a Binomial distribution, as:
\begin{itemize}
    \item The number of trials tends to infinity ($n \to \infty$)
    \item The mean number of successes remains fixed ($np = \lambda = \text{constant}$)
\end{itemize}

Given $\lambda$ as the mean number of expected events (per unit time) and $r$ as the number of events occurring in that time, it has PMF:
\[
    P(r; \lambda) = \frac{\exp(-\lambda) \lambda^r}{r!}
\]

And has the following properties:
\[
    \langle r\rangle = \lambda
\]
\[
    Var(r) = \lambda
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/lec05-02.png}
     \caption{The Poisson Distribution}
\end{figure}

\section{Normal Distribution}
A.K.A. The Gaussian distribution. This is the most well known and most useful distribution. Given a mean $\mu$ and a standard deviation $\sigma$, the probability density function is:
\[
    P(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(\frac{-(x - \mu)^2}{2 \sigma^2}\right)
\]

It is a very important distribution as it arises as a result of the Central Limit Theorem, which we will cover properly in the probability section of the course. It looks like this, noting it is symmetric and forms a ``bell-shaped curve'':
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/lec05-03.png}
     \caption{The Normal Distribution}
\end{figure}

Note that since the tails are logarithmic, they tend to zero, but never reach it truly. A Poisson distribution approaches a Normal distribution as $\lambda \to \infty$. It is generally a good approximation for $\lambda > 30$ but this depends on the application being used.\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/lec05-04.png}
     \caption{Poisson approximations to a Normal}
\end{figure}