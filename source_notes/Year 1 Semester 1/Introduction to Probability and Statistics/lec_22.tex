% !TeX root = main.tex
\lecture{P11}{Fri 12 Dec 2025 11:00}{\textbf{End of Probability}: Variance Propagation}

\section{Variance Propagation}

Say we have $x \sim P_x(x)$, but we don't know what the actual distribution $P_x(x)$ is. We have however been able to determine (or estimate) values for $\langle x\rangle$ and var$(x)$. We set $y = f(x)$, and what is $P_y(y)$? This is a common scenario in e.g. labs, where we can measure a sample mean and estimate a standard deviation, but we cannot necessarily actually determine the underlying distribution.

We would formally use:
\[
    P_y(y) = \left|\frac{d}{dy}f^{-1}(y)\right|P_x(x)(f^{-1}(y))
\]

But that is impossible here, as we do not know $P_x(x)$. We can however calculated values for $\langle y\rangle$ and var$(y)$.

\subsection{Example}
Imagine an experiment where we want to determine kinetic energy. This is difficult (as we would have to transfer this energy into another easier to measure form first). Instead, we measure velocity and use:
\[
    E = \frac{1}{2}mv^2 \quad \text{where $v$ is a random variable.}
\]

What is $\langle E\rangle$ and var$(E)$?

We use Taylor Series:
\[
    f(x) = f(x_0) + (x - x_0) f^\prime (x_0) + \frac{(x - x_0)^2}{2!} f^{\prime \prime} (x_0) + \cdots
\]
This generates a polynomial approximation for $f(x)$ about the point $x_0$. What if $x$ is a random variable? Lets assume $\langle x\rangle = \mu$ and $\text{var}(x) = \sigma^2$.

\[
    f(x) = f(\mu) + (x - \mu)f^\prime(\mu) + \frac{(x - \mu)^2}{2} f^{\prime\prime}(\mu) + \cdots
\]

We have effectively decoupled the $f$ and the $x$, allowing us to work with it more easily. We take expectations:
\[
    \langle f(x)\rangle \approx \langle f(\mu)\rangle + \langle(x - \mu)f^\prime(\mu)\rangle + \frac{1}{2}\langle(x - \mu)^2f^{\prime\prime}(\mu)\rangle+ \cdots
\]

\[
    \langle f(\mu)\rangle = f(\mu)
\]
\[
    \langle (x - \mu ) \rangle f^\prime(\mu) =  0
\]
\[
    \frac{1}{2}\langle (x - \mu ) f^{\prime\prime}(\mu)\rangle = \frac{\sigma^2}{2} f^{\prime\prime}(\mu)
\]

We know the second term is zero, and if we assume the third term and onwards are `small':
\[
    \langle f\rangle \approx f(\mu)
\]

We therefore use $f(\langle x\rangle)$ as an approximation for $\langle f\rangle$.

Now, for variance:
\[
    \text{var}(f) = \langle f^2\rangle - \langle f\rangle^2 \approx \langle f^2\rangle - (f(\mu))^2
\]

Approximating to the first order with $f(x) \approx f(\mu) + (x - \mu) f^\prime(\mu)$:

\[
    \langle f^2\rangle \approx \langle (f(\mu) + (x - \mu)f^\prime(\mu))^2\rangle - \langle f(\mu)^2\rangle + 2 f(\mu) f^\prime(\mu) \langle (x - \mu)\rangle + f^\prime(\mu)^2 \langle (x - \mu)^2\rangle
\]
\[
    \approx f(\mu)^2 + \sigma^2 f^\prime(\mu)^2
\]
\[
    \implies \text{var}(f) \approx \sigma^2 f^\prime(\mu)^2
\]

\textbf{Going back to the example:}

Say we've measured $\langle v\rangle = v_0$ and $\text{var}(v) = \sigma^2$. We want to estimate $\langle E\rangle$ and var$(E)$.

\[
    E = \frac{1}{2}mv^2
\]
\[
    \langle E\rangle \approx \frac{1}{2}m \langle v\rangle^2
\]
\[
    \text{var}(E) \approx \left(\frac{dE}{dv}\right)^2 \sigma^2
\]
\[
    = m^2 v^2 \sigma^2
\]

Here, $v$ is a `smooth' function with a constant second derivative, so the third term of the Taylor Expansion before is indeed small and this is a decent approximation, but it may not always be.

\section{Example II}
If $x$ is drawn according to a Poisson distribution with parameter $\lambda$, estimate the value of:
\[
    \left< \frac{1}{\sqrt{1+x}}\right>
\]
And:
\[
    \text{var} \left(\frac{1}{\sqrt{1+x}}\right)
\]

Note that:
\[
    \langle x\rangle = \text{var}(x) = \lambda
\]
\[
    \frac{d}{dx} \frac{1}{\sqrt{1+x}} = - \frac{1}{2(1+x)^{3/2}}
\]

From the definition of discrete expectation values:
\[
        \left< \frac{1}{\sqrt{1+x}}\right> = \sum_{x=0}^{\infty} \dfrac{1}{\sqrt{1+x}} \frac{\lambda^x}{x!} e^{-\lambda}
\]

This is\ldots unpleasant, so we need the approximation to reasonably determine it.
\[
    \langle f(x)\rangle \approx f(\langle x\rangle) = \frac{1}{\sqrt{1+ \langle x\rangle}} = \frac{1}{\sqrt{1 + \lambda}}
\]
\[
    \text{var}\left(\frac{1}{\sqrt{1+x}}\right) = \left(\frac{d}{dx} \frac{1}{\sqrt{1+x}}\right)^2 \text{var}(x)
\]
\[
    = \frac{1}{4(1+\lambda)^3} \lambda
\]

\section{General Formulae}
If we have N statistically independent random variables, $x_1, x_2, x_3, \ldots, x_n$ with:
\[
    \langle x_i\rangle = \mu_i \qquad \text{var}(x_i) = \sigma_i^2
\]

For some function $f(x_1, x_2, \ldots, x_n)$:
\[
    \langle f\rangle \approx f(\mu_1, \mu_2, \ldots, \mu_n)
\]
\[
    \text{var}(f) \approx \left(\frac{\partial f}{\partial x_1}\right)^2 \sigma_1^2+ \left(\frac{\partial f}{\partial x_2}\right)^2 \sigma_2^2+ \cdots + \left(\frac{\partial f}{\partial x_n}\right)^2 \sigma_n^2
\]

We estimate the values of $\mu_i, \sigma^2_i$ using sample values, i.e. in labs.

\subsection{Multivariate Example}
Again we want to determine kinetic energy, but this time we measure both $m$ and $v$ with:
\[
    \langle v\rangle = v_0 \qquad \langle m\rangle = m_0
\]
\[
    \text{var}(v) = \sigma_v^2 \qquad \text{var}(m) = \sigma_m^2
\]
\[
    E = \frac{1}{2}mv^2
\]

We again want to estimate $\langle E\rangle$ and var$(E)$.

\[
    \langle E\rangle \approx \frac{1}{2} \langle m\rangle\langle v\rangle^2 = \frac{1}{2}m_0 v_0^2
\]
\[
    \text{var}(E) \approx \left(\frac{dE}{dm}\right)^2 \text{var}(m) + \left(\frac{dE}{dv}\right)^2 \text{var}(v)
\]
\[
    = \frac{v_0^4}{4}\sigma^2_m + m_0^2 v_0^2 \sigma_v^2
\]

\begin{center}
    \vspace{1cm}
    \textbf{\large End of Module.}
\end{center} 
