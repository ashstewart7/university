% !TeX root = main.tex
\lecture{10}{Thu 30 Oct 2025 09:00}{Goodness of Fit}
Using likelihood, we can quantify how close our model is to the data, but how do we know if we've got the right model in the first place? We can use a \emph{goodness of fit statistic} to quantify this probabilistically.

\section{Straight Line Example}
Lets start by taking some data and fitting a straight line to it, using last lectures content:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec10-01.png}
     \caption{}
\end{figure}

We can calculate the $\chi^2$ value for this fit with:
\[
    \chi^2 = \sum_{i=1}^{n} \left(\frac{D_i - M(x_i, \theta)}{\sigma_{D_i}}\right)^2
\]
Where $M(x_i, \theta) = \hat{m}x_i + \hat{c}$. This gives $\chi^2 \approx 86.37$. We can generate many different datasets, and create lines of best fit for them, and calculate each fit's $\chi^2$. If we plot these values as a histogram we get:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec10-02.png}
     \caption{}
\end{figure}

This is called a ``$\chi^2$ distribution'' and is generated by the frequencies of $\chi^2$ values across many realisations of the data, if the model fitted to the data is the same model actually used to generate the data. A chi squared distribution can be described using the number of ``degrees of freedom'', $k$, and we denote a chi squared distribution with $k$ degrees as $\chi_k^2$. The PDF is given by:
 \[
     P(x; k) = \frac{1}{2^{k/2} \Gamma(k/2)}x^{k/2 - 1} e^{-x/2}
 \]
 
 Where $\Gamma(x)$ is the gamma function, an interpolation of the factorial function across all reals. The number of degrees of freedom is given by the number of data points minus the number of fitted parameters, i.e. $k = N - M$. Comparing out calculated distribution to the theoretical distribution given by the PDF gives:
 \begin{figure}[H]
     \centering
     \includegraphics[width=0.75\textwidth]{figures/lec10-03.png}
      \caption{}
 \end{figure}
 Which is a pretty good fit. We can also look at this distribution for different numbers of data points, where a larger number of data points gives a larger number of degrees of freedom:
 \begin{figure}[H]
     \centering
     \includegraphics[width=0.75\textwidth]{figures/lec10-04.png}
      \caption{}
 \end{figure}
 
If we are fitting the correct model to our data, then we know the distribution of expected $\chi^2$ values is (a $\chi^2_k$ distribution). This provides a goodness of fit statistic, by checking to see if the $\chi^2$ value from the best fit parameters is consistent with what we'd expect to see if our model was correctly chosen. 

If assume our model is correct, we can ask how likely it is that we'd get the data $\chi^2$ value from the $\chi^2_k$ likelihood. If this is reasonably likely, we say the goodness of fit is acceptable. If not, then we have a problem. This problem could be many things, including:
\begin{itemize}
    \item The incorrect model is being fitted - \emph{larger $\chi^2$ than expected}.
    \item The uncertainties on the data are too small to account for the observed noise - \emph{larger $\chi^2$ than expected}.
    \item The uncertainties on the data are too large so account for more than the actual noise - \emph{smaller $\chi^2$ than expected}.
    \item Something else has gone wrong\dots
\end{itemize}
The test itself cannot tell us exactly which of these is true, we have to use scientific judgement.

\section{Quantifying the Likelihood}
Since the $\chi^2_k$ distribution is continuous, the likelihood of getting a specific $\chi^2$ value is zero. Instead, we reframe and look at ``what is the chance of getting this value of $\chi^2$ or larger?''. This is done by:
\[
    P(\chi^2 \geq a) = \int_{a}^{\infty} P(\chi^2; k) \, d \chi^2
\]

Crucially, \textbf{the probability returned is only valid on the assumption that the model fitted to the data is correct.} If this value is above a certain threshold, we say this is evidence of a sensible fit. This is \textbf{not the chance that the model is correct, as the model is just that, a model, and is almost certainly never entirely correct.} It is the chance of getting this $\chi^2$ value or larger \emph{if} the model is correct.

Taking the previous example of $\chi^2 = 86.37$, with $k = 100-2 = 98$. This gives:
\[
    P(\chi^2 \geq 86.37) = \int_{86.37}^{\infty} P(\chi^2 ; 86.36) \, d \chi^2
\]
This needs to be evaluated numerically, as it becomes unpleasant for not-trivial values of $k$. This gives a probability of $0.793$. This is a reasonable value, not worryingly high (i.e. better than 99\%) or worryingly low (i.e. less than 1\%), so we say the fit is adequate. 

To counterexample, say we have a poor fit. The data is from $y = 0.85x^2 + 2x - 1$, and we try to fit a linear model to it:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec10-05.png}
     \caption{}
\end{figure}
This gives a $\chi^2$ value of 166.407. This is much larger than before, lets plot it on the distribution of the expected $\chi^2$:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec10-06.png}
     \caption{Probability is $1.98e-05$}
\end{figure}
So we can confidently say that the fit is not an acceptable quality fit. This is despite it not being a terrible fit by eye.