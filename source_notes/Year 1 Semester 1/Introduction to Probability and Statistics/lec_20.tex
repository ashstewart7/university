% !TeX root = main.tex
\lecture{P9}{Fri 05 Dec 2025 11:00}{Continuous Probability}
So far, we have only discussed discrete probability. In this, $P(x)$ means the probability of $x$ happening and we've ignored the possibility of an infinite number of possible events, because it never caused a problem.

\section{Continuous Distributions}
\subsection{Continuous Random Walk}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec20-01.png}
     \caption{The path taken by the particle in a random walk.}
\end{figure}

We have a particle conducting a random walk. What is the probability of the particle landing exactly on $(\pi, \pi)$? This is zero, and indeed it will be zero for every specific point we can name. 

We have to think about probability a bit differently, and we have to ask the question about the particle being \emph{near} $(\pi, \pi)$. We cannot consider exact values.

Continuous probability allows possible values to be real numbers (uncountably infinite) whereas before we were limited to discrete integers (countably infinite). $\Omega$ is therefore $\R$ or a subset thereof.

We have to set our question to be ``What is the probability that x lies in some interval''. Effectively, sums become integrals. $P(x)$ is now a probability density function, and the area under $P(x)$ is what gives us probability, rather than values of $P(x)$ alone.

\subsection{Properties of a PDF}
\begin{itemize}
    \item $P(x) \geq 0$:
    \begin{itemize}
        \item $P(x) = 0,\; \forall x \not \in \Omega$
        \item $P(x) > 0,\; \forall x \in \Omega$
    \end{itemize}
    
    \item It is normalised, hence:
    \[
        \int_{\Omega} P(x) \, dx = 1
    \]
    \item Since we care about the area under $P(x)$ to get probabilities, $P(x)$ itself may be bigger than 1, provided the integral is never bigger than 1.
    
\end{itemize}

All the old formulae still hold, but with integration instead of summation:
\begin{itemize}
    \item Expectation Values:
    \[
        \langle x\rangle = \int_{\Omega} xP(x) dx
    \]
    \item Expectation of a Function:
    \[
        \langle f\rangle =  \int_{\Omega} f(x) P(x) dx
    \]
    \item Variance:
    \[
        \text{var}(x) = \langle (x - \langle x\rangle)^2\rangle
    \]
    \[
        =  \int_{\Omega} (x - \langle x\rangle)^2 P(x)
    \]
    \[
        = \langle x^2\rangle - \langle x\rangle^2
    \]    
\end{itemize}

\section{Cumulative Distributions}
The definition of cumulative probability is the same:
\[
    C(x) \equiv \mathrm{Probability}(X \leq x)
\]

However we calculate it a bit differently as we can no longer simply sum the options below, we must integrate:
\[
    C(x) = \int_{-\infty}^{x} P(x) \, dx
\]
\[
    \frac{dC(x)}{dx} = P(x)
\]

We can also use this to determine the probability of $x$ being between two values, with:
\[
    P(a \leq x \leq b) = P(x \leq b) - P(x \leq a) = C(b) - C(a)
\]

\subsection{Example}
A PDF is given by:
\[
    P(x) = \frac{3}{2}(1 - x^2) \quad 0 \leq x \leq 1
\]

Find:
\[
    P\left(\frac{1}{4} \leq x \leq \frac{1}{2}\right)
\]

\[
        P\left(\frac{1}{4} \leq x \leq \frac{1}{2}\right) = C\left(\frac{1}{2}\right) - C\left(\frac{1}{4}\right)
\]
\[
    C(x) \equiv \int_{0}^{x} \frac{3}{2}(1-x^2)  \, dx
\]
\[
    = \frac{3x}{2} - \frac{x^3}{2}
\]

Hence $P\left(\frac{1}{4} \leq x \leq \frac{1}{2}\right) = C\left(\frac{1}{2}\right) - C\left(\frac{1}{4}\right)$ is easy (albeit faffy) to evaluate.

\section{Change of Variables}
As we did for discrete, let $x \sim P_x(x)$ and we set $y = f(x)$. What is $P_y(y)$?

This isn't needed for CoV to work, but for ease lets assume that $\Omega_x = [a, b]$, i.e. is some finite region. We need to conserve the normalisation, so:
\[
    1 = \int_{a}^{b} P_x(x) \, dx
\]

We use the fact that $y = f(x) \implies x = f^{-1}(y)$. This is true only for monotonic functions. For this to work, we need $f(x)$ to have a unique inverse and is either always increasing or always decreasing. We can treat this as an integration subsitution, so:
\[
    dx = \frac{df^{-1}y}{dy} dy \implies 1 = \int_{f(a)}^{f(b)} \frac{df^{-1}}{dy} P_x(f^{-1}(y)) \, dy \equiv \int_{\Omega_y} P_y(y) \, dy
\]

Where we call this new distribution the induced distribution. We have the general formula (again assuming monotonicity):
\[
P_y(y) = \left|\frac{d}{dy} f^{-1}(y)\right| P_x(f^{-1}(y))
\]
Note that the $| \dots |$ appear to ensure we handle the cases (where $f(x)$ is strictly increasing or strictly decreasing) as the strictly decreasing case will add an extra negative sign in the derivative. When integrating, we would swap the limits to cancel this extra negative, but we can't here as there's no integral, so taking the absolute value ensures that $P_y(y)$ is always positive.

 