% !TeX root = main.tex
\lecture{P8}{Thu 04 Dec 2025 09:00}{Multivariate Distributions}

For one variable, we can get $P(x)$:
\[
    \langle x\rangle = \sum_{x} x P(x)
\]

For multiple variables, we get the same thing a slightly different way by marginalising to find $P(x)$ and using that:
\[
    \langle x\rangle \equiv \sum_{x} \sum_{y} x P(x, y) = \sum_{x} x P(x)
\]

The two methods (direct sum including y or marginalising) are equivalent, they're just conceptually different.

\section{Sums of Random Variables}
We say $x$ was drawn from some distribution $P(x)$ by writing:
\[
    x \sim P(x)
\]
This tells us that $x$ is a random variable, and follows the distribution given after the $\sim$.

Consider $N$ random variables, all drawn from some distribution $P(x)$
\[
    x_1 \sim P(x) \quad x_2 \sim P(x)\ldots \quad x_n \sim P(x)
\]

We define the total:
\[
    t = x_1 + x_2 + x_3 + \cdots + x_n
\]

Working out $P(t)$ directly at this point is too difficult for us. We can however ask about the expected total $\langle t\rangle$ or var$(t)$. Recall the sample mean is:
\[
    \bar{x} = \frac{1}{N}(x_1 + \cdots + x_n) = \frac{t}{N}
\]

We already know expectation is linear, so $\langle x_1 + x+2 + \cdots + x_n\rangle = \langle x_1\rangle + \langle x_2\rangle + \cdots + \langle x_n\rangle$. 
The expectation value is given easily by:
\[
    \langle t\rangle = \langle x_1 + x+2 + \cdots + x_n\rangle
\]

For the variance, let's consider $N = 2$:
\[
    \text{var}(x_1 + x_2) = \langle (x_1 + x_2)^2\rangle + \langle x_1 + x_2\rangle^2
\]
\[
    = \langle x^2_1 + x^2_2 + 2x_1x_2\rangle - \left(\langle x_1 + x_2\rangle\right)^2
\]
\[
    \langle x_1^2\rangle + \langle x_2^2\rangle + 2 \langle x_1 x_2\rangle - \langle x_1\rangle^2 - \langle x_2\rangle^2 - 2 \langle x_1\rangle \langle x_2\rangle
\]
\[
    \langle x_1^2\rangle - \langle x_1\rangle^2 + \langle x^2_2\rangle - \langle x_2\rangle^2 + 2 \left(\langle x_1 x_2\rangle - \langle x_1\rangle - \langle x_2\rangle\right)
\]
\[
    \text{var}(x_1) + \text{var}(x_2) + 2 \text{cov}(x_1, x_2)
\]

Where $cov(x, y)$ is the covariance. 

\section{Covariance}
Recall that:
\[
    \text{var}(x) = \sum_{x} (x - \langle x\rangle)^2 P(x)
\]
We now define:
\[
    \text{cov}(x, y) = \sum_{xy} (x - \langle x\rangle)(y - \langle y\rangle)P(x, y)
\]
\[
    = \langle (x - \langle x\rangle)(y - \langle y\rangle)\rangle = \langle xy - x \langle y\rangle - y \langle x\rangle + \langle x\rangle \langle y\rangle\rangle
\]
\[
    = \langle xy\rangle - \langle y\rangle \langle x\rangle - \langle x\rangle \langle y\rangle + \langle x\rangle \langle y\rangle
\]
\[
    \langle xy\rangle - \langle x\rangle \langle y\rangle
\]

This is a generalisation for variance for multiple variables, so note that:
\[
    \text{var}(x) = \text{cov}(x, x)
\]

Covariance measures linear association between two variables:
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/lec19-01.png}
     \caption{}
\end{figure}

The caveat it's only linear is important - there may well be an association between two variables, but if it's not linear covariance won't test for it. For example, a true $n$-wavelengths of a sine wave will have zero covariance due to symmetry, but there is obviously an association (sinusoidal).

We define an additional measure called correlation using covariance (again, this is linear):
\[
    \text{corr}(x, y) = \frac{\text{cov(x, y)}}{\text{std}(x) \text{std}(y)}
\]
This is useful, because it standardises, i.e the maximum value for perfect linear association from the correlation function is $1$. This helps us to standardise regardless of units (i.e. if we were testing the correlation of two measurements with two different units, changing one of the units from km to mm would change the covariance. This is not helpful, as the actual underlying correlation stays the same. The corr function avoids this)

\section{Covariance and Independence}
If $x$ and $y$ are independent, then we know:
\[
    P(x, y) = P(x, y)
\]

How does this impact the covariance however?
\[
    \text{cov}(x, y) = \langle xy\rangle - \langle x\rangle \langle y\rangle
\]

Assume they are independent, then:
\[
    \langle xy\rangle = \sum_{x} \sum_{y} xy P(x) P(y)
\]
\[
    = \sum_{x} x P(x) \sum_{y} x P(y)
\]
\[
    = \langle x\rangle \langle y\rangle
\]

Hence \emph{if} they are independent, \emph{then} the covariance is zero (this does not go backwards).

\subsection{Variance of Sum}
If the variables are independent, the variance of the sum is the sum of the variances.

If they are not independent, we must include the covariance:
\[
    \text{var}(x_1 + x_2 + \cdots + x_n) = \sum_{n} + 2 \sum_{m > n} \text{cov}(x_n, x_m)
\]

As an example, what is the expectation value and the variance if we sum $N$ independent Bernoulli variables, each with the same parameter $p$.
\[
    x_1 \sim \text{Bern}(p) \qquad x_2 \sim \text{Bern}(p) \qquad \cdots \qquad x_n \sim \text{Bern}(p)
\]


For expectation values:
\[
    \langle x_1 + x_2 + \cdots + x_n\rangle = \sum_{i=1}^{N} \langle x_n\rangle = Np
\]

And for variance:
\[
    \text{var}(x_1 + \cdots + x_n) = \sum_{i=1}^{N} \text{var}(x_n) = Np(1-p)
\]

This is our results for the binomial distribution, as expected. Since we assume independence here, this is why the binomial distribution requires independent trials.

\section{Change of Variables (Discrete)}
We have some $P_x(x)$, in a sample space $\Omega_x$. We make a transformation of x,  $y = f(x)$.

For example, we have a particle moving with random velocity, and we want to the random variable that models kinetic energy. Kinetic energy is the new post-transformation random variable - the ``induced distribution''.

What then is $P_y(y)$ or $\Omega_y$?

Consider a fair six-sided die:

\[P_x(x) = 1/6, \quad x = 1, 2, 3, 4, 5, 6\]

\textbf{Example 1: $y = x - 2$}
This changes the sample space but not the distribution shape itself. This is simple, as it's a bijection:
\[
    P_y(y) = \frac{1}{6} \qquad y= -1, 0, 1, 2, 3, 4
\]

\textbf{Example 2: $z = |x - 2|$}
This is a bit tricker, because it is not a bijection. Two different values of $x$ ($x = 1, 3$) both map to $z=1$. We therefore need to consider both cases to ensure the new PMF is still normalised:
\[P_z(z) = \begin{cases*}
    2/6 \quad z = 1 \; (x = 1, 3)\\
    1/6 \quad z = {0, 2, 3, 4} \; (x = 2, 4, 5, 6)
\end{cases*}\]

The general formula for a particular case of $P_y(y)$ is:
\[
    P_y(y) = \sum_{x:y = f(x)} P_x(x)
\]
The $x:y = f(x)$ notation means that we are summing over values of $x$ such that $y = f(x)$. In english, the formula is saying ``to find the probability of the new variable having value $y$, sum the probabilities for all the $x$es that transform into that value of $y$''.

$\Omega_y$ is the unique set of values that arise from $y = f(x), \; \forall x \in \Omega_x$.
