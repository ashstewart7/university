% !TeX root = main.tex
\lecture{P4}{Thu 20 Nov 2025 09:00}{Conditional Probability}
\section{Axioms of Probability}

For a sample space $\Omega$, a distribution $P(x)$ must satisfy:
\begin{enumerate}
    \item $P(x) \geq 0$ for any $x \in \Omega$ (for discrete events)
    \item $P(\Omega) = 1$
    \item $P(e_1 \cup e_2 \cup + \cdots \cup e_n) = P(e_1) + P(e_2) + \cdots + P(e_n)$ 
    \begin{itemize}
        \item if the elements are pairwise disjoint ($e_i \cap e_j = \emptyset, i \neq j$).
    \end{itemize}
\end{enumerate}

\section{Conditional Probability}
\subsection{Example}
{
    \centering
    \emph{Throw two dice. What is the probability that we see a 4, given the total was 6?}
    \par
}
{
    \raggedright
    This ``given that'' is the key. It provides us with an extra piece of information that the final probability depends on.
    \par
}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/lec15-01.png}
     \caption{The sample space.}
\end{figure}

We can now see that our probability is $\frac{2}{5}$. This is different than if we had not considered the extra information, and would have drastically changed our answer.

\subsection{Definition}
The conditional probability of A given B is written $P(A \mid B)$ and is defined by:
\[
    P(A \mid B) \equiv \frac{P(A \cap B)}{P(B)} = \frac{\text{number of events in A and B}}{\text{number of events in B}} \qquad P(B) \neq 0
\]

This is the fraction of events in $B$ where both $A$ and $B$ happen.

\subsection{Verifying This is Still a Probability}
It may not be obvious that this is still a valid probability (i.e. that taking ratios of probabilities still yields a probability).

\begin{proof}
Assume that P is a valid probability function and let $Q(A \mid B) \equiv \dfrac{P(A \cap B)}{P(B)}$

\textbf{Consider the first axiom of probability:}

$Q(A \mid B) \geq 0$. This is satisfied, as $P(x) \geq 0$ for any $x \in \Omega$.

\textbf{Consider the second:}

$Q(\Omega \mid B) = \dfrac{P(\Omega \cap B)}{P(B)}=\dfrac{P(B)}{P(B)} = 1$ so satisfied.

\textbf{Consider the third:}

If $a_1 \cap a_2 = \emptyset$, is $Q(a_1 \cup a_2 \mid B) = Q(a_1 \mid B) + Q(a_2 \mid B)$?
\begin{figure}[H]
    \centering
    \includegraphics[width=0.25\textwidth]{figures/lec15-02.png}
     \caption{}
\end{figure}
The definition of Q gives:

\begin{align*}
    Q(a_1 \cup a_2 \mid B) &= \dfrac{P([a_1 \cup a_2] \cap B)}{P(B)} \\
    &= \dfrac{P([a_1 \cap B] \cup [a_2 \cap B])}{P(B)} \\
    &= \dfrac{P(a_1 \cap B) + P(a_2 \cap B)}{P(B)} \\
    &= \frac{P(a_1 \cap B)}{P(B)} + \frac{P(a_2 \cap B)}{P(B)} \\
    &= Q(a_1 \mid B) + Q(a_2 \mid B)
\end{align*}

So yes, the third axiom is satisfied.

\emph{Since all three axioms are satisfied, $Q$, hence $P$, is a valid probability distribution.}
\end{proof}

\section{Reconditioning}
Reconditioning is using $P(A \mid B)$ to determine $P(B \mid A)$.

\[
    (1) \quad P(A \mid B) = \frac{P(A \cap B)}{P(B)}
\]
\[
    (2) \quad P(B \mid A) = \frac{P(B \cap A)}{P(A)}
\] 

Using the fact that $A \cap B = B \cap A$:
\[
    P(B \mid A) = \frac{P(A \cap B)}{P(A)}
\]
\[
    \implies P(A \cap B) = P(A \mid B)P(B) = P(B \mid A)P(A)
\]
\[
    \boxed{\implies P(B \mid A) = \frac{P(A \mid B)P(B)}{P(A)}}
\]

\subsection{Example}
A rare cold-like disease has symptoms with probability $0.95$. The probability in the population of having the disease is $0.0001$. The probability of having the cold-like symptoms in the population is $0.4$, either from the disease or any other cold.

Given someone has symptoms, what is the probability they have the disease? Let $d$ be having the disease, and let $s$ be having symptoms. Then:
\[
    P(d) = 0.0001
\]
\[
    P(s) = 0.4
\]
\[
    P(s \mid d) = 0.95
\]

Therefore:
\[
    P(d \mid s) = \frac{P(s \mid d)P(d)}{P(s)}
\]
\[
    = \frac{0.95 \times 0.0001}{0.4} = 0.002
\]

So, as expected, it is very unlikely for someone to have the disease even if they present with symptoms.

\section{Statistical Independence}
If knowing B happened has no impact on whether A happens or not, then:
\[
    P(A \mid B) = P(A)
\]

In this case:
\[
    P(A \cap B) = P(A \mid B) P(B) = P(A)P(B)
\]
\[
    P(x_1 \cap x_2 \cap \cdots \cap x_n) = P(x_1)P(x_2)\cdots P(x_n)
\]


This (if it is true) is called \emph{statistical independence}.